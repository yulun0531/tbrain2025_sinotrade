import pandas as pd
import os
import numpy as np
import lightgbm as lgb
import joblib
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, log_loss
from imblearn.over_sampling import SMOTE
from collections import Counter

# è¨­å®šæ•¸æ“šå­˜æ”¾è·¯å¾‘
output_dir = r'C:\Users\yulun\Downloads\38_Training_Data_Set\Split_Files'

# è®€å–ä¸¦åˆä½µ CSV æ–‡ä»¶
df_list = []
for i in range(1, 6):
    file_path = os.path.join(output_dir, f'data{i}.csv')
    df = pd.read_csv(file_path, encoding="utf-8-sig")
    df_list.append(df)
    print(f"ç¬¬ {i} å€‹æª”æ¡ˆè¼‰å…¥æˆåŠŸ")
merged_df = pd.concat(df_list, ignore_index=True)

# æª¢æŸ¥ä¸¦å¡«è£œ NaN å€¼
if merged_df.isnull().sum().sum() > 0:
    print("ç™¼ç¾ NaN å€¼ï¼Œé–‹å§‹å¡«è£œ...")
    print(f"ğŸ›‘ æ¸¬è©¦è³‡æ–™æœ‰NANï¼Œç¼ºå°‘: {merged_df.isnull().sum().sum()}å€‹")
    merged_df.fillna(0, inplace=True)
    print("NaN å¡«è£œå®Œæˆï¼")

# ç§»é™¤éæ•¸å€¼å‹æ¬„ä½
for col in merged_df.select_dtypes(include=['object']).columns:
    print(f"ç§»é™¤éæ•¸å€¼æ¬„ä½: {col}")
    merged_df.drop(columns=[col], inplace=True)

# ç¢ºèªç›®æ¨™è®Šæ•¸å­˜åœ¨
if 'é£†è‚¡' not in merged_df.columns:
    raise ValueError("æ‰¾ä¸åˆ°ç›®æ¨™è®Šæ•¸ 'é£†è‚¡'ï¼Œè«‹ç¢ºèªè³‡æ–™é›†æ˜¯å¦æ­£ç¢ºï¼")

# åˆ†é›¢ç‰¹å¾µèˆ‡ç›®æ¨™è®Šæ•¸
X = merged_df.drop('é£†è‚¡', axis=1)
y = merged_df['é£†è‚¡']

# è½‰æ›ç‚º numpy array
X = X.to_numpy()
y = y.to_numpy()

# è¨ˆç®—æ­£è² æ¨£æœ¬æ•¸é‡
num_pos = np.sum(y == 1)
num_neg = np.sum(y == 0)
scale_pos_weight = num_neg / num_pos
print(f"æ­£æ¨£æœ¬æ•¸é‡: {num_pos}, è² æ¨£æœ¬æ•¸é‡: {num_neg}, è¨ˆç®—å¾—åˆ°çš„ scale_pos_weight: {scale_pos_weight:.4f}")

# è¨­å®šäº¤å‰é©—è­‰
n_splits = 5
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# **6ï¸âƒ£ è¨“ç·´æ¨¡å‹**
params = {
        'boosting_type': 'gbdt',
        'objective': 'binary',
        'metric': 'binary_logloss',
        'num_leaves': 31,  #å› ç‚ºç‰¹å¾µè¼ƒè¤‡é›œå˜—è©¦æé«˜
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'min_data_in_leaf': 20,
        'scale_pos_weight': scale_pos_weight,
        'device': 'gpu',
        'gpu_platform_id': 0,
        'gpu_device_id': 0,
        'verbose': -1
    }
## æ’°å¯«è¨“ç·´ç”¨çš„åƒæ•¸
params = {
        'task': 'train',
        ## ç›®æ¨™å‡½æ•¸
        'boosting_type': 'gbdt',
        'objective': 'binary',
        'metric': 'binary_logloss',
        ## ç®—æ³•é¡å‹
        'num_leaves': 50,
        'max_depth': 6,
        'max_bin' : 511,
        'learning_rate': 0.06,
        ## æ§‹å»ºæ¨¹æ™‚çš„ç‰¹å¾µé¸æ“‡æ¯”ä¾‹
        'feature_fraction': 0.8,
        'feature_fraction_seed': 42,
        "bagging_fraction":0.8,
        ## k è¡¨ç¤ºæ¯kæ¬¡è¿­ä»£å°±é€²è¡Œbagging
        'bagging_freq':5,
        "min_child_samples": 20,
        'min_data_in_leaf': 60,
        'lambda_L1': 0.6,
        'lambda_L2': 0.6,
        ## å¦‚æœæ•¸æ“šé›†æ¨£æœ¬åˆ†å¸ƒä¸å‡è¡¡ï¼Œå¯ä»¥å¹«åŠ©æ˜é¡¯æé«˜æº–ç¢ºç‡
        'scale_pos_weight': scale_pos_weight,
        'verbose':-1,
}
auc_scores = []
accuracy_scores = []
f1_scores = []
precision_scores = []
recall_scores = []
train_log_losses = []
val_log_losses = []
models = []

# **3ï¸âƒ£ é€²è¡Œ K-Fold è¨“ç·´**
for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    print(f"\nğŸš€ æ­£åœ¨è¨“ç·´ç¬¬ {fold+1} æŠ˜...")
    
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
    print(f"ğŸ¯ è¨“ç·´é›†ä¸­å„é¡åˆ¥åˆ†å¸ƒ: {Counter(y_train)}")

    # **5ï¸âƒ£ æº–å‚™ LGBM æ•¸æ“šé›†**
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

    gbm = lgb.train(
        params,
        train_data,
        valid_sets=[train_data, valid_data],
        num_boost_round=500,
        callbacks=[
            lgb.early_stopping(stopping_rounds=25),
            lgb.log_evaluation(period=5)
        ]
    )

    # **7ï¸âƒ£ é æ¸¬èˆ‡è©•ä¼°**
    y_pred_train = gbm.predict(X_train)
    y_pred = gbm.predict(X_val)

    y_pred_train_binary = (y_pred_train > 0.3).astype(int)
    y_pred_binary = (y_pred > 0.3).astype(int)

    # **8ï¸âƒ£ è¨ˆç®—è©•ä¼°æŒ‡æ¨™**
    train_log_loss = log_loss(y_train, y_pred_train)
    val_log_loss = log_loss(y_val, y_pred)

    auc = roc_auc_score(y_val, y_pred)
    accuracy = accuracy_score(y_val, y_pred_binary)
    f1 = f1_score(y_val, y_pred_binary)
    precision = precision_score(y_val, y_pred_binary)
    recall = recall_score(y_val, y_pred_binary)
    print(f"é æ¸¬æ©Ÿç‡ç¯„åœ: {np.min(y_pred):.4f} ~ {np.max(y_pred):.4f}")
    print(f"é æ¸¬å¤§æ–¼ 0.3 çš„æ¯”ä¾‹: {np.mean(y_pred > 0.3):.4f}")

    print(f"âœ… ç¬¬ {fold+1} æŠ˜çµæœ - AUC: {auc:.4f}, æº–ç¢ºç‡: {accuracy:.4f}, F1-score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}")
    print(f"ğŸ› ï¸ è¨“ç·´ Log Loss: {train_log_loss:.4f}, é©—è­‰ Log Loss: {val_log_loss:.4f}")
    
    # **9ï¸âƒ£ å„²å­˜è©•ä¼°çµæœ**
    auc_scores.append(auc)
    accuracy_scores.append(accuracy)
    f1_scores.append(f1)
    precision_scores.append(precision)
    recall_scores.append(recall)
    train_log_losses.append(train_log_loss)
    val_log_losses.append(val_log_loss)
    models.append(gbm)

# **ğŸ”Ÿ é¡¯ç¤ºæœ€çµ‚å¹³å‡çµæœ**
print("\nğŸ“Š K-Fold äº¤å‰é©—è­‰çµæœ (å¹³å‡å€¼):")
print(f"ğŸ¯ å¹³å‡ AUC: {sum(auc_scores) / n_splits:.4f}")
print(f"ğŸ¯ å¹³å‡ æº–ç¢ºç‡: {sum(accuracy_scores) / n_splits:.4f}")
print(f"ğŸ¯ å¹³å‡ F1-score: {sum(f1_scores) / n_splits:.4f}")
print(f"ğŸ¯ å¹³å‡ Precision: {sum(precision_scores) / n_splits:.4f}")
print(f"ğŸ¯ å¹³å‡ Recall: {sum(recall_scores) / n_splits:.4f}")
print(f"ğŸ› ï¸ å¹³å‡ è¨“ç·´ Log Loss: {sum(train_log_losses) / n_splits:.4f}")
print(f"ğŸ› ï¸ å¹³å‡ é©—è­‰ Log Loss: {sum(val_log_losses) / n_splits:.4f}")

# **ğŸ† æª¢æ¸¬éæ“¬åˆ**
if min(auc_scores) < 0.85:  # è‹¥æŸæŠ˜é©—è­‰ AUC éä½
    print("âš ï¸ å¯èƒ½éæ“¬åˆï¼å»ºè­°æ¸›å°‘ `num_leaves` æˆ–å¢åŠ  `feature_fraction`ã€‚")

best_model_index = np.argmax(auc_scores)
best_model = models[best_model_index]
model_path = r'C:\Users\yulun\Downloads\lightgbm_best_model.pkl'
joblib.dump(best_model, model_path)
print(f"æœ€ä½³æ¨¡å‹å·²å„²å­˜è‡³ {model_path}")

feature_names_path = r'C:\Users\yulun\Downloads\feature_names.txt'
np.savetxt(feature_names_path, merged_df.columns[:-1], fmt='%s', encoding='utf-8')
print(f"ç‰¹å¾µåç¨±å·²å„²å­˜è‡³ {feature_names_path}")

importance_path = r'C:\Users\yulun\Downloads\feature_importance.csv'
pd.DataFrame({
    'Feature': merged_df.columns[:-1],
    'Importance': best_model.feature_importance(importance_type='gain')
}).sort_values(by='Importance', ascending=False).to_csv(importance_path, index=False, encoding='utf-8-sig')
print(f"ç‰¹å¾µé‡è¦æ€§å·²å„²å­˜è‡³ {importance_path}")