import pandas as pd
import os
import numpy as np
import lightgbm as lgb
import joblib
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, log_loss
from sklearn.feature_selection import mutual_info_classif
from imblearn.over_sampling import SMOTE
from collections import Counter

# è¨­å®šæ•¸æ“šå­˜æ”¾è·¯å¾‘
output_dir = r'C:\Users\yulun\Downloads\tbrain2025_sinotrade\Split_Files'
mi_scores_path = r'C:\Users\yulun\Downloads\mi_scores.csv'
feature_importance_path = r'C:\Users\yulun\Downloads\feature_importance.csv'

# è®€å–ä¸¦åˆä½µ CSV æ–‡ä»¶
df_list = []
for i in range(1, 22):
    file_path = os.path.join(output_dir, f'data{i}.csv')
    df = pd.read_csv(file_path, encoding="utf-8-sig")
    df_list.append(df)
    print(f"ç¬¬ {i} å€‹æª”æ¡ˆè¼‰å…¥æˆåŠŸ")
merged_df = pd.concat(df_list, ignore_index=True)

# æª¢æŸ¥ä¸¦å¡«è£œ NaN å€¼
if merged_df.isnull().sum().sum() > 0:
    print("ç™¼ç¾ NaN å€¼ï¼Œé–‹å§‹å¡«è£œ...")
    merged_df.fillna(0, inplace=True)
    print("NaN å¡«è£œå®Œæˆï¼")

# ç§»é™¤éæ•¸å€¼å‹æ¬„ä½
for col in merged_df.select_dtypes(include=['object']).columns:
    merged_df.drop(columns=[col], inplace=True)

# ç¢ºèªç›®æ¨™è®Šæ•¸å­˜åœ¨
if 'é£†è‚¡' not in merged_df.columns:
    raise ValueError("æ‰¾ä¸åˆ°ç›®æ¨™è®Šæ•¸ 'é£†è‚¡'ï¼Œè«‹ç¢ºèªè³‡æ–™é›†æ˜¯å¦æ­£ç¢ºï¼")

# åˆ†é›¢ç‰¹å¾µèˆ‡ç›®æ¨™è®Šæ•¸
X = merged_df.drop('é£†è‚¡', axis=1)
y = merged_df['é£†è‚¡']

# è¨ˆç®—äº’ä¿¡æ¯
#print("è¨ˆç®—äº’ä¿¡æ¯")
#mi_scores = mutual_info_classif(X, y, discrete_features=False)
#mi_df = pd.DataFrame({'Feature': X.columns, 'MI_Score': mi_scores})
#mi_df = mi_df.sort_values(by='MI_Score', ascending=False)

# è®€å– LightGBM ç‰¹å¾µé‡è¦æ€§
feature_importance_df = pd.read_csv(feature_importance_path, encoding='utf-8-sig')
mi_df = pd.read_csv(mi_scores_path, encoding='utf-8-sig')
# ç¢ºä¿æ¬„ä½åç¨±ä¸€è‡´
feature_importance_df.rename(columns={'Feature': 'Feature', 'Importance': 'LGBM_Score'}, inplace=True)
mi_df.rename(columns={'Feature': 'Feature', 'MI_Score': 'MI_Score'}, inplace=True)
mi_df = mi_df.sort_values(by='MI_Score', ascending=False)

# è¨­å®šç‰¹å¾µé‡è¦æ€§æ’åï¼ˆæ•¸å€¼è¶Šå¤§ä»£è¡¨è¶Šé‡è¦ï¼‰
feature_importance_df['LGBM_Rank'] = feature_importance_df['LGBM_Score'].rank(ascending=False, method='average')
mi_df['MI_Rank'] = mi_df['MI_Score'].rank(ascending=False, method='average')

# åˆä½µå…©å€‹ DataFrame
merged_df = pd.merge(feature_importance_df, mi_df, on='Feature', how='inner')

# è¨ˆç®—ç¶œåˆæ’åï¼ˆå¯ä»¥ç”¨å¹³å‡æˆ–åŠ æ¬Šæ–¹å¼ï¼‰
merged_df['Final_Rank'] = (merged_df['LGBM_Rank'] + merged_df['MI_Rank']) / 2  # å¹³å‡æ’å
# merged_df['Final_Rank'] = 0.7 * merged_df['LGBM_Rank'] + 0.3 * merged_df['MI_Rank']  # åŠ æ¬Šæ’åï¼ˆLightGBM æ¬Šé‡é«˜ï¼‰

# æ ¹æ“šæ’åæ’åº
merged_df = merged_df.sort_values(by='Final_Rank', ascending=True)

# é¸æ“‡å‰ top_k_features å€‹ç‰¹å¾µ
top_k_features = 100  # å¯æ ¹æ“šéœ€æ±‚èª¿æ•´
selected_features = merged_df.iloc[:top_k_features]['Feature'].tolist()
X = X[selected_features]

# è½‰æ›ç‚º numpy array
X = X.to_numpy()
y = y.to_numpy()

# è¨ˆç®—æ­£è² æ¨£æœ¬æ•¸é‡
num_pos = np.sum(y == 1)
num_neg = np.sum(y == 0)
scale_pos_weight = num_neg / num_pos

# è¨­å®šäº¤å‰é©—è­‰
n_splits = 5
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# è¨“ç·´åƒæ•¸
params = {
    'task': 'train',
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'num_leaves': 50,
    'max_depth': 6,
    'max_bin': 511,
    'learning_rate': 0.06,
    'feature_fraction': 0.8,
    'feature_fraction_seed': 42,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'min_child_samples': 20,
    'min_data_in_leaf': 60,
    'lambda_L1': 0.6,
    'lambda_L2': 0.6,
    'scale_pos_weight': scale_pos_weight,
    'verbose': -1,
}

# åˆå§‹åŒ–è©•ä¼°æŒ‡æ¨™
auc_scores, accuracy_scores, f1_scores, precision_scores, recall_scores, models = [], [], [], [], [], []

# é€²è¡Œ K-Fold è¨“ç·´
for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    print(f"\nğŸš€ æ­£åœ¨è¨“ç·´ç¬¬ {fold+1} æŠ˜...")
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
    
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

    gbm = lgb.train(
        params,
        train_data,
        valid_sets=[train_data, valid_data],
        num_boost_round=500,
        callbacks=[
            lgb.early_stopping(stopping_rounds=30),
            lgb.log_evaluation(period=5)
        ]
    )

    # é æ¸¬èˆ‡è©•ä¼°
    y_pred = gbm.predict(X_val)
    y_pred_binary = (y_pred > 0.3).astype(int)

    auc = roc_auc_score(y_val, y_pred)
    accuracy = accuracy_score(y_val, y_pred_binary)
    f1 = f1_score(y_val, y_pred_binary)
    precision = precision_score(y_val, y_pred_binary)
    recall = recall_score(y_val, y_pred_binary)

    print(f"âœ… ç¬¬ {fold+1} æŠ˜çµæœ - AUC: {auc:.4f}, æº–ç¢ºç‡: {accuracy:.4f}, F1-score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}")

    auc_scores.append(auc)
    accuracy_scores.append(accuracy)
    f1_scores.append(f1)
    precision_scores.append(precision)
    recall_scores.append(recall)
    models.append(gbm)

# **ğŸ”Ÿ é¡¯ç¤ºæœ€çµ‚å¹³å‡çµæœ**
print("\nğŸ“Š K-Fold äº¤å‰é©—è­‰çµæœ (å¹³å‡å€¼):")
print(f"ğŸ¯ å¹³å‡ AUC: {sum(auc_scores) / n_splits:.4f}")
print(f"ğŸ¯ å¹³å‡ æº–ç¢ºç‡: {sum(accuracy_scores) / n_splits:.4f}")
print(f"ğŸ¯ å¹³å‡ F1-score: {sum(f1_scores) / n_splits:.4f}")
print(f"ğŸ¯ å¹³å‡ Precision: {sum(precision_scores) / n_splits:.4f}")
print(f"ğŸ¯ å¹³å‡ Recall: {sum(recall_scores) / n_splits:.4f}")

# **ğŸ† æª¢æ¸¬éæ“¬åˆ**
if min(auc_scores) < 0.85:  # è‹¥æŸæŠ˜é©—è­‰ AUC éä½
    print("âš ï¸ å¯èƒ½éæ“¬åˆï¼å»ºè­°æ¸›å°‘ `num_leaves` æˆ–å¢åŠ  `feature_fraction`ã€‚")
# å„²å­˜æœ€ä½³æ¨¡å‹
best_model_index = np.argmax(auc_scores)
best_model = models[best_model_index]
model_path = r'C:\Users\yulun\Downloads\lightgbm_best_model.pkl'
joblib.dump(best_model, model_path)
print(f"æœ€ä½³æ¨¡å‹å·²å„²å­˜è‡³ {model_path}")

# å„²å­˜é¸å®šç‰¹å¾µ
selected_features_path = r'C:\Users\yulun\Downloads\selected_features.txt'
np.savetxt(selected_features_path, selected_features, fmt='%s', encoding='utf-8')
print(f"é¸å®šçš„ç‰¹å¾µå·²å„²å­˜è‡³ {selected_features_path}")

# å„²å­˜äº’ä¿¡æ¯æ•¸å€¼
mi_scores_path = r'C:\Users\yulun\Downloads\mi_scores.csv'
mi_df.to_csv(mi_scores_path, index=False, encoding='utf-8-sig')
print(f"äº’ä¿¡æ¯æ•¸æ“šå·²å„²å­˜è‡³ {mi_scores_path}")